{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torchvision.transforms import transforms\n",
    "import pathlib\n",
    "import nibabel as nib\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WE MOETEN NOG DROPOUT IMPLEMENTEREN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_voor = []\n",
    "Train_na = []\n",
    "\n",
    "path = pathlib.Path('processed').parent\n",
    "for timestamp in [\"-001h\", \"024h\"]:\n",
    "    for mouse in [\"M03\", \"M04\", \"M05\", \"M06\", \"M07\"]:\n",
    "        if timestamp == \"-001h\":\n",
    "            path_ct = path / f\"processed/{mouse}_{timestamp}_CT280.img\"\n",
    "            Train_voor.append(nib.load(path_ct).get_fdata())\n",
    "        else: \n",
    "            path_ct = path / f\"processed/{mouse}_{timestamp}_CT280.img\"\n",
    "            Train_na.append(nib.load(path_ct).get_fdata())           \n",
    "\n",
    "Train_Data_001h = []\n",
    "Train_Data_024h = []\n",
    "for mouse in Train_voor:\n",
    "    for slice in mouse:\n",
    "        Train_Data_001h.append(slice)\n",
    "\n",
    "for mouse in Train_na:\n",
    "    for slice in mouse:\n",
    "        Train_Data_024h.append(slice)\n",
    "\n",
    "\n",
    "Test_Data_001h = []\n",
    "Test_Data_024h = []\n",
    "\n",
    "for timestamp in [\"-001h\", \"024h\"]:\n",
    "    mouse = \"M08\"\n",
    "    path_ct = path / f\"processed/{mouse}_{timestamp}_CT280.img\"\n",
    "    ct = nib.load(path_ct).get_fdata()\n",
    "    for slice in ct:\n",
    "        if timestamp == \"-001h\":\n",
    "            Test_Data_001h.append(slice)\n",
    "        else:\n",
    "            Test_Data_024h.append(slice)\n",
    "\n",
    "#Train_Data_001h (770 slices) 5 muisjes\n",
    "#Train_Data_024h (770 slices)\n",
    "#Test_Data_001h  (154 slices)\n",
    "#Test_Data_024h  (154 slices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module): #dit is 1 blok van 2 convs gevolgd door een relu\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels) #is dit noodzakelijk en waarom doet men dit en moet dit voor of na conv?\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    " \n",
    "        self.relu = nn.ReLU() #evt leaky ReLu??\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.float()\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)        \n",
    "        return x\n",
    "\n",
    "class res_block(nn.Module): #dit is 1 blok van 2 convs gevolgd door een relu\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels) #is dit noodzakelijk en waarom doet men dit en moet dit voor of na conv?\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    " \n",
    "        self.relu = nn.ReLU() #evt leaky ReLu??\n",
    "\n",
    "        ###SKIP CONNECTION (Identity Mapping)\n",
    "        self.s = nn.Conv2d(in_channels,out_channels,kernel_size=1,padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.float()\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        s = self.s(inputs)\n",
    "        \n",
    "        return x + s\n",
    "\n",
    "\n",
    "class encoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = res_block(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.float()\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "\n",
    "        return x, p\n",
    "\n",
    "class decoder_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv_block(in_channels, out_channels) #heb ik hier het juiste aantal channels???\n",
    "\n",
    "    def forward(self, inputs, skip):\n",
    "        inputs = inputs.float()\n",
    "        skip = skip.float()\n",
    "        x = self.up(inputs)\n",
    "        _, _, H, W = x.shape\n",
    "        skip = torchvision.transforms.CenterCrop([H,W])(skip)\n",
    "        x = torch.cat([skip, x], axis=1) #ik heb ook al meer advanced versies van de resizing gezien, maakt dit veel uit?\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() #residuals nog implementeren.\n",
    "\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.e1 = encoder_block(1, 64)\n",
    "        self.e2 = encoder_block(64, 128)\n",
    "        self.e3 = encoder_block(128, 256)\n",
    "        self.e4 = encoder_block(256, 512)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.b = res_block(512, 1024) # hoe beslis je eig hoeveel features je wilt per layer?\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.d1 = decoder_block(1024, 512)\n",
    "        self.d2 = decoder_block(512, 256)\n",
    "        self.d3 = decoder_block(256, 128)\n",
    "        self.d4 = decoder_block(128, 64)\n",
    "\n",
    "        \"\"\" Last layer, i.e. de eigenlijke voorspelling \"\"\"\n",
    "        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        s1, p1 = self.e1(inputs)\n",
    "        s2, p2 = self.e2(p1)\n",
    "        s3, p3 = self.e3(p2)\n",
    "        s4, p4 = self.e4(p3)\n",
    "\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.b(p4)\n",
    "\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        d1 = self.d1(b, s4)\n",
    "        d2 = self.d2(d1, s3)\n",
    "        d3 = self.d3(d2, s2)\n",
    "        d4 = self.d4(d3, s1)\n",
    "\n",
    "        outputs = self.outputs(d4)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(device)\n",
    "optimizer = Adam(model.parameters(),lr=0.01,weight_decay=0.001)\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "batch_size = 6\n",
    "# 6 muizen(5training; 1 testing), 2 time instances, 154 slices, (242,121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.RandomHorizontalFlip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MuizenDataset(Dataset):\n",
    "\n",
    "    def __init__(self,data_voor,data_na,p=0.5,pad=True):\n",
    "        super().__init__()\n",
    "        self.data_voor = (data_voor - np.mean(data_voor))/np.std(data_voor) #vanwege de kleine dataset laden we het gewoon helemaal in memory en normaliseren we in place\n",
    "        self.data_na = (data_na - np.mean(data_na))-np.std(data_na)\n",
    "        self.p = p\n",
    "        self.pad = pad\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_voor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        input = self.data_voor[index]\n",
    "        target = self.data_na[index]\n",
    "        input = torch.tensor(input)\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        if torch.rand(1) < self.p:\n",
    "            input = torchvision.transforms.functional.hflip(input)\n",
    "            target = torchvision.transforms.functional.hflip(target)\n",
    "        \n",
    "        #we willen onze afbeeldingen ook wat scheef inlezen soms omdat we dan overfitting kunnen vermijden\n",
    "        degrees = np.random.randint(-10,10,size=None)\n",
    "        input = torchvision.transforms.functional.rotate(input.unsqueeze(0),degrees)\n",
    "        target = torchvision.transforms.functional.rotate(target.unsqueeze(0),degrees)\n",
    "        #input = torchvision.transforms.functional.rotate(input,50)\n",
    "        #target = torchvision.transforms.functional.rotate(target,50)\n",
    "        #print(input)\n",
    "        #print(target)\n",
    "        #if self.pad and torch.rand(1) < self.p:\n",
    "         #   input = torchvision.transforms.Pad((0,15), fill=0, padding_mode='constant')(input)\n",
    "          #  target = torchvision.transforms.Pad((0,15), fill=0, padding_mode='constant')(target)\n",
    "\n",
    "        return input.float(), target.float()\n",
    "\n",
    "    \n",
    "\n",
    "#onze laatste batch is incomplete dus deze laten we vallen\n",
    "train_loader = DataLoader(MuizenDataset(Train_Data_001h,Train_Data_024h),batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(MuizenDataset(Test_Data_001h,Test_Data_024h),batch_size=batch_size,shuffle=True,drop_last=True)\n",
    "\n",
    "#len(MuizenDataset(Train_Data_001h,Train_Data_024h,transform=transformer))\n",
    "#list(MuizenDataset(Train_Data_001h,Train_Data_024h,transform=transformer))\n",
    "#plt.imshow(MuizenDataset(Train_Data_001h,Train_Data_024h,transform=transformer)[100][1],cmap='bone')\n",
    "print(len(train_loader)) # hoeveel batches je hebt gemaakt van lengte batch_size\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):  # we itereren meerdere malen over de data tot convergence?\n",
    "    print(\"epoch: {}\".format(epoch))\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, (batch_voor, batch_na) in enumerate(tqdm(train_loader)): #wat is een handige manier om dit in te lezen?\n",
    "        #dim = batch_voor.shape #nu kunnen we ook verschillende input dimensies afhandelen\n",
    "        \n",
    "        \n",
    "        #batch_voor = batch_voor.view(batch_size,1,dim[1],dim[2])\n",
    "        #batch_na = batch_na.view(batch_size,1,dim[1],dim[2])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            batch_voor=Variable(batch_voor.cuda())\n",
    "            batch_na=Variable(batch_na.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        predicted_batch = model(batch_voor)\n",
    "        _, _, H, W = predicted_batch.shape\n",
    "        batch_na = torchvision.transforms.CenterCrop([H,W])(batch_na)\n",
    "\n",
    "        afb_pred = predicted_batch[5][0].cpu()\n",
    "        afb_voor = batch_voor[5][0].cpu()\n",
    "        afb_na = batch_na[5][0].cpu()\n",
    "        \n",
    "        loss = loss_function(predicted_batch,batch_na) #vergelijk predicted na image met de echte na image\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if i%10==0: #efjes afgezet\n",
    "            plt.figure(figsize=(20,10))\n",
    "            plt.subplot(1,3,1)\n",
    "            plt.imshow(afb_voor.detach().numpy(),cmap='bone')\n",
    "            plt.title('Voor injectie')\n",
    "            plt.subplot(1,3,2)\n",
    "            plt.imshow(afb_na.detach().numpy(),cmap='bone')\n",
    "            plt.title('Na injectie')\n",
    "            plt.subplot(1,3,3)\n",
    "            plt.imshow(afb_pred.detach().numpy(),cmap='bone')\n",
    "            plt.title('Predictie')\n",
    "            plt.show()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "#plt.title('Losses')\n",
    "#plt.plot(losses)\n",
    "#plt.show()\n",
    "\n",
    "    #print('Average loss:' + str(epoch_loss/len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODEL OPSLAAN EN OPNIEUW LADEN\n",
    "#Doe dit om memory te besparen en de GPU te clearen\n",
    "model_path = \"model_TEST.pth\"\n",
    "torch.save(model.state_dict(),model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "device = torch.device('cpu')\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TESTING\n",
    "model.eval()\n",
    "test_acc = []\n",
    "for i, (batch_voor,batch_na) in enumerate(tqdm(test_loader)):\n",
    "    batch_voor = batch_voor.view(batch_size,1,121,242)\n",
    "    batch_na = batch_na.view(batch_size,1,121,242)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        batch_voor=Variable(batch_voor.cuda())\n",
    "        batch_na=Variable(batch_na.cuda())\n",
    "    \n",
    "    predicted_batch = model(batch_voor)\n",
    "\n",
    "    _, _, H, W = predicted_batch.shape\n",
    "    batch_na = torchvision.transforms.CenterCrop([H,W])(batch_na)\n",
    "\n",
    "    loss = loss_function(predicted_batch,batch_na) #vergelijk predicted na image met de echte na image\n",
    "    test_acc.append(loss)\n",
    "\n",
    "    if i%10==0:\n",
    "        print('Step: '+ str(i)+'loss: '+str(loss))\n",
    "av_test_acc = np.mean(np.array(test_acc))\n",
    "print(av_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1477f09f345c54836698d61586a11c7930607b6c64dd8e48e86789b527ef112b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('VOPENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
